---
title: "Estero Bay Shell Height Sample Size"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

CZ325 Task 3: Evaluation of sample size to assess managed-area-level trends Deliverable 3a:

```{r}
library(cmdstanr)
library(rstan)
library(posterior)
library(bayesplot)
library(magrittr)
library(tidyverse)
library(spBayes)
library(MBA)
#library(geoR)
library(fields)
library(sp)
library(maptools)
library(rgdal)
library(classInt)
library(lattice)
library(RColorBrewer)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

library(leaflet)
library(raster)
library(ggmap)
library(sf)
```

The goal here is to use a spatial model for oyster shell heights by reef. The model will be used to predict mean shell heights at unseen locations 
and prediction uncertainty will be used to guide additional samples.
Look at some real data
```{r}
oysterraw_2022_06_30 = readRDS(here::here("CZ325_Sample_Size_task_3/oysterraw_2022-06-30_wcoords.rds"))
```
We use UniversalReefID as a unique identifier of a location.

First, we look at shell height as a response at single managed area, Estero Bay.
There are multiple (many) shells per site.
```{r}
oysterraw_2022_06_30 %>% dplyr::select(ShellHeight_mm, ManagedAreaName,SampleDate,ShellHeight_mm, ObsIndex, Latitude, Longitude,UniversalReefID) %>% na.omit() %>% filter(ManagedAreaName == "Estero Bay Aquatic Preserve") -> Estero_Bay_shell_height
```

Looks like 58 sites, i.e., unique UniversalReefID's

```{r}
Estero_Bay_shell_height %>% group_by(UniversalReefID) %>% summarize( n = n())
```

There are several lat/lons associated with a single 
use average UniversalReefID lat lon for the position of UniversalReefID
so to get a single lat lon for a single UniversalReefID, we average the lat lons

This data set has multiple shell height observations per UniversalReefID site
ShellHeight_mm ManagedAreaName             SampleDate          ObsIndex Latitude Longitude UniversalReefID mean_lat mean_lon DaysSince group

We also compute a "DaysSince" field. This represents the number of days since the olded shell was sampled. It can be used to 
model growth. It is not used in this analysis.

```{r}
Estero_Bay_shell_height %>% dplyr::select( UniversalReefID, Latitude,Longitude) %>%  group_by(UniversalReefID) %>% summarize( mean_lat = mean(Latitude), mean_lon = mean(Longitude) ) %>% distinct() -> UI2LL
left_join(Estero_Bay_shell_height,UI2LL,by = join_by(UniversalReefID)) %>% mutate(DaysSince = (SampleDate - min(SampleDate))/3600/24 ) %>% group_by(UniversalReefID) %>% mutate( group = cur_group_id()) -> Estero_Bay_Data
```

We use stan for Bayesian model fitting. Stan is a separate language for model description. There are several packages
that can read and compile stan code. cmdstanr is the latest and greatest. rstan is another one. There is also pystan if Python is your language
of choice.

This takes about an hour and dumps out a lot of status on the way.

The model is descibed in the report but it basically models the covariance of shell heights as a function
of distance. Sites that are close together have a higher covariance than things far apart. Covariance
descreases exponentially. 

Fit the Bayesian model for
mean shell heights that vary by space

Total execution time: 3893.9/3600 = 1.08
```{r}

Data = list( N =  nrow(Estero_Bay_Data)
             , y = Estero_Bay_Data$ShellHeight_mm
             , x = as.numeric(Estero_Bay_Data$DaysSince) # this can be used for growth calculations
             , G = max(Estero_Bay_Data$group)
             , group = Estero_Bay_Data$group
             , positions = (Estero_Bay_Data %>% dplyr::select(mean_lon, mean_lat, group) %>% distinct() %>% arrange(group))[,c("mean_lon","mean_lat")]
             )
              
 
spatial_mean_RE.mod <- cmdstan_model(here::here("CZ325_Sample_Size_task_3/spatial_mean_RE.stan"))

 spatial_mean_RE.EsteroBay.Shell_Height =  spatial_mean_RE.mod$sample(
    data = Data,
    seed = 1111, 
    chains = 4, # 8
    parallel_chains = 4, # 8
    refresh = 1000,
    iter_warmup = 5000, 
    iter_sampling = 5000,
    thin =  1
    #, adapt_delta = 0.99
  )
```

Look to see how well the model fits the data by plotting the average mean height with the predicted mean height.
```{r}
data.frame(empirical_mean = (Estero_Bay_Data %>% group_by(UniversalReefID) %>% summarize( empirical_mean = mean(ShellHeight_mm)))$empirical_mean, fitted_mean = spatial_mean_RE.EsteroBay.Shell_Height$summary("site_mean")$mean) %>% 
  ggplot(aes(x = empirical_mean, y = fitted_mean)) + geom_point() + geom_smooth() + ggtitle("Estero Bay")
```


Standard Deviation of posterior distribution of mean shell_height_mm estimate per site as a function of sample size
```{r}
sd_by_sample_size = data.frame(sample_size = (Estero_Bay_Data %>% group_by(UniversalReefID) %>% summarize( n = n()))$n, sd = spatial_mean_RE.EsteroBay.Shell_Height$summary("site_mean")$sd)
```

In the report, we explain how the plot on the left is showing uncertainty as a function of sample size by
simply using the classical standard deviation of the average and the plot on the right shows the standard deviations
of the posterior predictions, which uses spatial information. The key insight is that the plot on the right is smoother
and that selecting sample size from a spatially informed model is apt to provide a better estimate of uncertainty.
```{r}
rbind(Estero_Bay_Data %>% group_by(UniversalReefID) %>% summarize( sample_size = n(), sd = sd(ShellHeight_mm)/sqrt(n())) %>% dplyr::select(sample_size, sd) %>% mutate(method = "empirical"),
sd_by_sample_size %>% mutate(method = "spatial model")) %>% ggplot(aes(x = sample_size, y = sd, color = method)) + geom_point() + geom_smooth( show.legend = FALSE) + facet_wrap(~method) + theme(legend.position="none")

```

```{r}
ggsave("sd_by_sample_size.png", height = 5)
```


Density of shell heights
```{r}
Estero_Bay_Data %>% ggplot(aes(x = ShellHeight_mm)) + geom_histogram()
```


Regular grid inside the convex hull of the original points.
Using functions from R-package sp, we get a grid of n = 256 points within the convex hull
of our existing 58 sites. We use this to predict from our existing points to points
in the grid. 


```{r}
grid = spsample(Polygon(Data$positions[chull(Data$positions),]), n = 256, type = "regular")
new_positions = grid@coords
```

Prediction: draw from the relevant posterior distributions.
cmdstanr has a method "draws" to extract the posterior samples of each estimated parameter.
The model has alpha, rho, and the gp_function, which is the spatial random effects variable.
The accompanying report shows the math and the intuition, but essentially it is the variable
that is representing the spatial dependence.

We also use a number of helper functions to find the distances between all the new points and the old ones
and between all the new points. All these are fed into a prediction function

```{r}
source(here::here("CZ325_Sample_Size_task_3/SpatialPredictionUtilities.R"))
alpha_posterior = spatial_mean_RE.EsteroBay.Shell_Height$draws("sigma", format = "draws_matrix") # 4 chains 5000 draws after 5000 warmup
rho_posterior = spatial_mean_RE.EsteroBay.Shell_Height$draws("length_scale", format = "draws_matrix")
obs_gp_function_posterior = spatial_mean_RE.EsteroBay.Shell_Height$draws("gp_function", format = "draws_matrix")
obs_distances =  my_calc_point_distances(Data$positions)
pred_distances =  my_calc_point_distances(new_positions)
obs_to_pred_distances = my_calc_point_distances(Data$positions, new_positions)
global_mean_posterior = spatial_mean_RE.EsteroBay.Shell_Height$draws("global_mean", format = "draws_matrix")
```


For each draw on all the posteriors of all the parameters, make a prediction of the mean shell height on all
the new points. We do this 100 times, so essentially simulating the uncertainty associated with the predictions.
At the end of this, we have 100 predictions of the mean shell heights at all the new positions.

```{r}
mean_preds <- matrix(NA, nrow = 100, ncol = NROW(new_positions))
for(i in 1:NROW(mean_preds)){
  #cat('Processing posterior draw', i, 'of 5...\n')
  mean_preds[i,] <- predict_mean(as.numeric(global_mean_posterior[i,]), obs_distances = obs_distances,
                             pred_distances = pred_distances,
                             obs_to_pred_distances = obs_to_pred_distances,
                             obs_gp_function = as.vector(as.matrix(obs_gp_function_posterior[i,])),
                             alpha = alpha_posterior[i],
                             rho = rho_posterior[i])
}
```

Plot of sd of estimates. Without any of the map info, you can see how the uncertainty in the prediction
basically is larger the farther you get away from the know points.
```{r}
data.frame(mean_lon = new_positions[,1], mean_lat = new_positions[,2], sd = apply(mean_preds, 2, sd) ) %>% ggplot(aes(x = mean_lon, y = mean_lat, color = sd, size = sd)) + geom_point() + geom_point(data = Data$positions, aes(x = mean_lon, y = mean_lat, size = 1), color = "red", show.legend = FALSE)
```

ggmap by default uses Google maps. To use this, you need to register to get a key and then use that for your plotting session.

```{r}
ggmap(get_map(location = as.data.frame(new_positions), zoom = 12))+ geom_point( data = as.data.frame(new_positions), aes(x =x1, y = x2 , color = apply(mean_preds,2,sd)),size = apply(mean_preds,2,sd)) + geom_point( data = Data$positions, aes(x =mean_lon, y = mean_lat ), color = "red")  +  guides(color = guide_legend(title = "sd"))
ggsave("Estero Bay grid plot.png")
```


```{r}
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=Data$positions$mean_lon, lat=Data$positions$mean_lat, color = "red", radius = 0.5) %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=new_positions[,1], lat=new_positions[,2], radius = 4*apply(mean_preds,2,sd))
m
```

```{r}
cz325_GRMAPandEBAPoysters = readRDS("cz325_GRMAPandEBAPoysters.rds")
```

Prediction at additional oyster sites -- look at the ones in the Estero Bay managed area.
We also get the "geometry" which is actually a polygon outlining the reef.
We use a function st_centroid from the R-package 'st' to get the centroid of each polygon.
This is because our model is point based and we are looking for an estimate at a location.
Then we do everything like we did with the grid: get the posteriors of all the paramaters in the model,
get all the distances between the new oyster positions and the distances between all the new positions
and the old ones.

The we draw on the posterior distributions, use those and the distances to predict at all the new oyster locations.
We do this 100 times and when it is done, we have 100 estimates of the mean shell height at all the new oyster positions.
The standard deviation of the mean oyster predictions represents our measure in uncertainty.

```{r}
hey = cz325_GRMAPandEBAPoysters[ cz325_GRMAPandEBAPoysters$METADATA == "oysters_estero_2021","geometry"]
#get centroids
oyster_positions = as.data.frame(st_coordinates(st_centroid(hey$geometry)))
names(oyster_positions) = c("lon","lat")

obs_to_pred_distances = my_calc_point_distances(Data$positions, oyster_positions)
pred_distances =  my_calc_point_distances(oyster_positions)
oyster_mean_preds <- matrix(NA, nrow = 100, ncol = NROW(oyster_positions))
for(i in 1:NROW(oyster_mean_preds)){
  #cat('Processing posterior draw', i, 'of 5...\n')
  oyster_mean_preds[i,] <- predict_mean(as.numeric(global_mean_posterior[i,]), obs_distances = obs_distances,
                             pred_distances = pred_distances,
                             obs_to_pred_distances = obs_to_pred_distances,
                             obs_gp_function = as.vector(as.matrix(obs_gp_function_posterior[i,])),
                             alpha = alpha_posterior[i],
                             rho = rho_posterior[i])
}

```
As with the grid, you can see that prediction uncertainty increases as one moves away from existing sample points were the model was fitted.
```{r}
ggmap(get_map(location = as.data.frame(oyster_positions), zoom = 12))+ geom_point( data = as.data.frame(oyster_positions), aes(x =lon, y = lat , color = apply(oyster_mean_preds,2,sd)),size = apply(oyster_mean_preds,2,sd)) + geom_point( data = Data$positions, aes(x =mean_lon, y = mean_lat ), color = "red")  +  guides(color = guide_legend(title = "sd"))
```
```{r}
ggsave("OysterMap.png")
```
Leaflet version
```{r}
mO <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=oyster_positions$lon, lat=oyster_positions$lat, radius = 4*apply(oyster_mean_preds,2,sd))  %>%
  addCircleMarkers(lng=Data$positions$mean_lon, lat=Data$positions$mean_lat, color = "red", radius = 0.5)
mO
```


```{r}
plot(density(apply(mean_preds,2,sd)))
```




This another way to view the uncertainty spatially. We have an emprirical relationship 
between the standard deviation at one of the sample points and the number of samples taken there.

We use a semi-parametric model to relate standard deviation to sample size. We then can use this to predict the sample size 
at any point with the prediction uncertainty. 


```{r}
fit = loess(sample_size ~ sd, sd_by_sample_size)

```


Effective sample sizes: for each predicted sd, compute the sample size that would have been required to achieve it.
```{r}
n_hat = as.numeric(predict(fit, newdata = data.frame(sd = apply(mean_preds, 2, sd)))) 
data.frame(mean_lon = new_positions[,1], mean_lat = new_positions[,2], n_hat ) %>% ggplot(aes(x = mean_lon, y = mean_lat, color = n_hat, size = n_hat)) + geom_point() + geom_point(data = Data$positions, aes(x = mean_lon, y = mean_lat, size = 1), color = "red")
```

```{r}
ggmap(get_map(location = as.data.frame(new_positions), zoom = 12))+ geom_point( data = as.data.frame(new_positions), aes(x =x1, y = x2 , color = n_hat),size = 0.002*n_hat) + geom_point( data = Data$positions, aes(x =mean_lon, y = mean_lat ), color = "red")  +  guides(color = guide_legend(title = "effective sample sizes"))
```

```{r}
m1 <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=Data$positions$mean_lon, lat=Data$positions$mean_lat, color = "red", radius = 0.005*(Estero_Bay_Data %>% count(UniversalReefID))$n, label = (Estero_Bay_Data %>% count(UniversalReefID))$n) %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=new_positions[,1], lat=new_positions[,2], radius = 0.005*n_hat, label = round(n_hat))
m1
```


Generalized random tessellation sampling (GRTS) is a defacto standard in envirommental sampling.
There are numerous refences to the method (we cite some in the report). Here we use GRTS which
is avaliable in the R-package spsurvey. The way we use it here, is to generate a design with 
a certain number of sample points. The function grts() does all the work. You need to give it the
possible sampling locations. It has an algorithm that tries to find a spatially representative 
sample of sites. But the addition here is to also use it to prefer points where our model
is uncertain. This informs the model to select points that represent locations where 
we are uncertain and need more information. 

The variable we use for preference, that is the probability of inclusion in the sample, is proportional to the variance
of the predictions


Proportional inclusion probabilities
Do sd of average size:
nonUniform sampling using GRTS


To understand how this effects sample size, we do a simulation at various sample sizes, say from 2 to 50.
grts() generates random samples, so we do this a number (100) times. For each sample_size, we use the variance of the predictions
of all the possible oyster positions (that is, positions and variances) in the grts() function, use it to sample sample_size positions, 
predict the mean shell height at those positions (100 times as before), compute the of all the predictions. The idea here is that 
the goal might be to estimate the mean shell height for the entire managed area. So we simply use the standard deviations of all the
esimates of the mean shell height. For each sample_size draw from grts(), we get a standard deviation of mean shell height. 
One would expect that the standard deviations of the standard deviations would decrease as sample_size increase.









Proportional inclusion probabilities
Do sd of average size:
nonUniform sampling using GRTS
# adding prediction uncertainty to sampling
```{r}
require(spsurvey) 
oyster.positions.sd.sf =st_as_sf(data.frame(oyster_positions, sd = apply(oyster_mean_preds,2,sd), var  = apply(oyster_mean_preds,2,var)), coords = c("lon","lat"), crs =  st_crs("WGS84"))

GRTS_nonuniform_sampling_results = data.frame() 
for( sample_size in 2:50){
  
  for( iterations in 1:100){
    
    tmp_new_positions = as.data.frame(st_coordinates(grts(oyster.positions.sd.sf,n_base = sample_size, projcrs_check = FALSE, aux_var = "var")$sites_base))
    
    #tmp_new_positions = runif_in_polygon(n = sample_size, vertices = as.matrix(Data$positions[chull(Data$positions),]))
obs_to_pred_distances = my_calc_point_distances(Data$positions, tmp_new_positions)
pred_distances =  my_calc_point_distances(tmp_new_positions)
tmp_mean_preds <- matrix(NA, nrow = 100, ncol = NROW(tmp_new_positions))
for(i in 1:NROW(tmp_mean_preds)){
  #cat('Processing posterior draw', i, 'of 5...\n')
  tmp_mean_preds[i,] <- predict_mean(as.numeric(global_mean_posterior[i,]), obs_distances = obs_distances,
                             pred_distances = pred_distances,
                             obs_to_pred_distances = obs_to_pred_distances,
                             obs_gp_function = as.vector(as.matrix(obs_gp_function_posterior[i,])),
                             alpha = alpha_posterior[i],
                             rho = rho_posterior[i])
       }
  GRTS_nonuniform_sampling_results = rbind(GRTS_nonuniform_sampling_results, data.frame(sample_size = sample_size, sd = sd(apply(tmp_mean_preds,1,mean))))
  }
}

```
In this plot, we see how overall mean shell height prediction improves as sample size increases using a combination
of grts()  and site selection using prediction uncertainty.

```{r}
GRTS_nonuniform_sampling_results %>% ggplot(aes(x = sample_size, y = sd)) + geom_jitter(width = 0.1, alpha = 0.2)  + geom_smooth() + ggtitle("Sample inclusion probability proportional to estimation variance")
ggsave("sd_by_spatial_sampl.png", height = 5)
```

The basic spatial model can incorporate covariates to (potentially) improve prediction and if so, reduce sample sizes.
That is, if one can accurately predict at unseen sites, there is no need to sample there. Or sample less frquently.

We show an example, incorporating Salinity as a covariate. Salinity doesn't really predict shell height very well so 
we go though the analysis formally to show how it's done.

First, read in some salinity data and filter out only those that correspond to our sites.

```{r}
cz325_t3_salinity = read.csv(here::here("CZ325_Sample_Size_task_3/cz325_t3_salinity.csv"))
cz325_t3_salinity %>% filter( ManagedAreaName == "Estero Bay Aquatic Preserve") -> EsteroBay_cz325_t3_salinity
```

We just need the locations and values.
```{r}
EsteroBay_cz325_t3_salinity %>% dplyr::select(OriginalLongitude,OriginalLatitude) -> salinity_locations
EsteroBay_cz325_t3_salinity %>% dplyr::select(OriginalLongitude,OriginalLatitude, ResultValue ) -> salinity_values
```

For each point in original data find closest position and corresponding salinity.
We use a distance calculation function (in our 'SpatialPredictionUtilities.R' file) and do a nearest neightbors calculation.
```{r}
orig_to_salinity_distances = my_calc_point_distances(Data$positions, salinity_locations)
original_salinity_values = salinity_values$ResultValue[apply(orig_to_salinity_distances,1,which.min)]

oyster_to_salinity_distances = my_calc_point_distances(oyster_positions, salinity_locations)
oyster_salinity_values = salinity_values$ResultValue[apply(oyster_to_salinity_distances,1,which.min)]
```

Note that the correlation is pretty weak and the prediction is not apt to work well.
Correlation between average shell height and salinity
```{r}
plot(original_salinity_values, (Estero_Bay_Data%>% group_by(UniversalReefID) %>% summarize(m = mean(ShellHeight_mm)))$m)
```

We have a version of the stan code that can incorporate a covariate.
It relates mean shell height to the salinity at the site.


Need stan code to use covariate
Fit the Bayesian model for
Mean shell heights that vary by space; salinity is a covariate for each site
Total execution time: 609.2 seconds
```{r}

Data = list( N =  nrow(Estero_Bay_Data)
             , y = Estero_Bay_Data$ShellHeight_mm
             , x = original_salinity_values # 
             , G = max(Estero_Bay_Data$group)
             , group = Estero_Bay_Data$group
             , positions = (Estero_Bay_Data %>% dplyr::select(mean_lon, mean_lat, group) %>% distinct() %>% arrange(group))[,c("mean_lon","mean_lat")]
             )
              
 
spatial_RE.mod <- cmdstan_model(here::here("CZ325_Sample_Size_task_3/spatial_RE.stan")) # allows a covariate on the site mean

spatial_salinity_RE.EsteroBay.Shell_Height =  spatial_RE.mod$sample(
  data = Data,
  seed = 1111, 
  chains = 4, # 8
  parallel_chains = 4, # 8
  refresh = 1000,
  iter_warmup = 5000, 
  iter_sampling = 5000,
  thin =  1
  #, adapt_delta = 0.99
)
```
Just as before, calculate the standard deviation of posterior distribution of mean shell_height_mm estimate per site as a function of sample size
Standard Deviation of posterior distribution of mean shell_height_mm estimate per site as a function of sample size
```{r}
sd_by_sample_size_covariate = data.frame(sample_size = (Estero_Bay_Data %>% group_by(UniversalReefID) %>% summarize( n = n()))$n, sd = spatial_mean_RE.EsteroBay.Shell_Height$summary("site_mean")$sd, sd_w_covariate = spatial_salinity_RE.EsteroBay.Shell_Height$summary("site_mean")$sd)
```

```{r}
sd_by_sample_size_covariate %>% ggplot(aes(x = sample_size, y = sd)) + geom_point()+ geom_smooth() + ggtitle("Estero Bay Shell Height")
```


As before, need posterior distributions and distances between new points and between new points and old ones.
```{r}
#Prediction: draw from the relevant posterior distributions

source(here::here("CZ325_Sample_Size_task_3/SpatialPredictionUtilities.R"))
alpha_covariate_posterior = spatial_salinity_RE.EsteroBay.Shell_Height$draws("sigma", format = "draws_matrix") # 4 chains 5000 draws after 5000 warmup
rho_covariate_posterior = spatial_salinity_RE.EsteroBay.Shell_Height$draws("length_scale", format = "draws_matrix")
obs_gp_function_posterior = spatial_salinity_RE.EsteroBay.Shell_Height$draws("gp_function", format = "draws_matrix")
obs_distances =  my_calc_point_distances(Data$positions)
pred_distances =  my_calc_point_distances(oyster_positions)
obs_to_pred_distances = my_calc_point_distances(Data$positions, oyster_positions)
slope_posterior = spatial_salinity_RE.EsteroBay.Shell_Height$draws("slope", format = "draws_matrix")
intercept_posterior = spatial_salinity_RE.EsteroBay.Shell_Height$draws("intercept", format = "draws_matrix")
#global_mean_posterior = spatial_mean_RE.EsteroBay.Shell_Height$draws("global_mean", format = "draws_matrix")
```

And now do all the predictions as before. Take draws on the posteriors of the parameters, feed in the distances and predict
100 times for each new position. We don't show the results because strictly speaking, the relationship between salinity and 
mean shell height is poor (R^2 of 0.005)

```{r}
mean_covariate_preds <- matrix(NA, nrow = 100, ncol = NROW(oyster_positions))
for(i in 1:NROW(mean_covariate_preds)){
  #cat('Processing posterior draw', i, 'of 5...\n')
  global_mean_covariate_posterior = intercept_posterior[i] + slope_posterior[i] *original_salinity_values
  mean_covariate_preds[i,] <- predict_covariate_mean(as.numeric(global_mean_covariate_posterior), obs_distances = obs_distances,
                             pred_distances = pred_distances,
                             obs_to_pred_distances = obs_to_pred_distances,
                             obs_gp_function = as.vector(as.matrix(obs_gp_function_posterior[i,])),
                             alpha = alpha_covariate_posterior[i],
                             rho = rho_covariate_posterior[i])
}
```


